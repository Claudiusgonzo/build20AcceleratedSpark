{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Queries on a Structured Stream\n",
    "\n",
    "We have 106GB of NYC Yellow Taxi data that has been cleaned and normalized based on the [published schema](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localsparkeast.internal.cloudapp.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fead4b4e2d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"readTaxi\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_file_dir=os.path.expanduser(\"~/data/taxi_data_cleaned_18_standard/\")\n",
    "bad_records_path=os.path.expanduser(\"~/data/badrecords\")\n",
    "taxi_output_dir=os.path.expanduser(\"~/data/query_output\")\n",
    "checkpoint_loc=os.path.expanduser(\"~/data/checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define schema ahead of time for structured streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_orig_schema = (\n",
    "    StructType()\n",
    "    .add(\"VendorID\", IntegerType())\n",
    "    .add(\"tpep_pickup_datetime\", TimestampType())\n",
    "    .add(\"tpep_dropoff_datetime\", TimestampType())\n",
    "    .add(\"passenger_count\", IntegerType())\n",
    "    .add(\"trip_distance\", DoubleType())\n",
    "    .add(\"RatecodeID\", IntegerType())\n",
    "    .add(\"store_and_fwd_flag\", StringType())\n",
    "    .add(\"PULocationID\", IntegerType())\n",
    "    .add(\"DOLocationID\", IntegerType())\n",
    "    .add(\"payment_type\", IntegerType())\n",
    "    .add(\"fare_amount\", DoubleType())\n",
    "    .add(\"extra\", DoubleType())\n",
    "    .add(\"mta_tax\", DoubleType())\n",
    "    .add(\"tip_amount\", DoubleType())\n",
    "    .add(\"tolls_amount\", DoubleType())\n",
    "    .add(\"improvement_surcharge\", DoubleType())\n",
    "    .add(\"total_amount\", DoubleType())\n",
    "    .add(\"congestion_surcharge\", DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stream Source\n",
    "\n",
    "Also get the SQL table reference for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_input_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .schema(taxi_orig_schema)\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .option(\"badRecordsPath\", bad_records_path)\n",
    "    .csv(taxi_file_dir) #the source\n",
    ")\n",
    "\n",
    "#Create SQL table out of it\n",
    "taxi_input_df.createOrReplaceTempView('nyyellowtaxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "```sql\n",
    "1. select passenger_count, year(tpep_pickup_datetime) year from nyyellowtaxi\n",
    "2. select passenger_count, avg(total_amount) as avg_amount from nyyellowtaxi group by passenger_count\n",
    "3. select passenger_count, year(tpep_pickup_datetime) year, count(*) total \n",
    "   from nyyellowtaxi group by passenger_count, year\n",
    "4. select passenger_count, year(tpep_pickup_datetime) as year, \n",
    "    cast(trip_distance as int) as distance, \n",
    "    count(*) as total \n",
    "    from nyyellowtaxi\n",
    "    group by passenger_count, year, distance\n",
    "    order by year, total desc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"select passenger_count, year(tpep_pickup_datetime) year from nyyellowtaxi\",\n",
    "          \"select passenger_count, avg(total_amount) as avg_amount from nyyellowtaxi group by passenger_count\",\n",
    "           \"select passenger_count, year(tpep_pickup_datetime) year, count(*) total FROM $tableName group by passenger_count, year\",\n",
    "           \"\"\"select passenger_count, year(tpep_pickup_datetime) as year, \n",
    "        cast(trip_distance as int) as distance, \n",
    "        count(*) as total \n",
    "        from $tableName\n",
    "        group by passenger_count, year, distance\n",
    "        order by year, total desMc\"\"\"\n",
    "          ]\n",
    "\n",
    "query_names = [\"q{}\".format(i) for i in range(1, len(queries))]\n",
    "out_dirs = [os.path.join(taxi_output_dir, n) for n in query_names]\n",
    "checkpoint_locs = [os.path.join(checkpoint_loc, n) for n in query_names]\n",
    "name_query_outdir = list(zip(query_names, queries, out_dirs, checkpoint_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_query(query_desc):\n",
    "  query_name, query_str, output_dir, checkpoint_loc = query_desc\n",
    "  q = (\n",
    "    spark.sql(query_str)\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", output_dir)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"checkpointLocation\", checkpoint_loc)\n",
    "    .queryName(query_name)\n",
    "    .trigger(processingTime='1 seconds')    \n",
    "    .start()\n",
    "  )\n",
    "  return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = start_query(name_query_outdir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
